#! /usr/bin/env python
# -*- coding: utf-8 -*-
'''Simulate tree'''
import os
try:
    import cPickle as pickle
except:
    import pickle
from nestly import Nest
from nestly.scons import SConsWrap
# the following must be exported by parent SConstruct/SConscript
Import('env tool_dict quick idlabel outdir exp_data naiveIDexp mutability substitution CommandRunner xarg buffarg')


clone_dict = dict()
with open(exp_data) as fh:
    header = fh.next()
    # print('Header:', header)
    for l in fh:
        cloneID, seqID, abundance, iso_set, chain, seq = l.strip().split(',')
        if len(cloneID) > 9:
            raise Exception('The cloneID cannot exceed 9 characters.')
        # Convertions:
        seqID = seqID + '_' + chain
        abundance = int(abundance)
        iso_set = set(iso_set.split(':')) if iso_set != '' else {}
        if not cloneID in clone_dict:
            clone_dict[cloneID] = dict()
        if not seqID in clone_dict[cloneID]:
            clone_dict[cloneID][seqID] = {'abundance':abundance, 'iso_set':iso_set, 'chain':chain, 'seq':seq}
        else:
            raise Exception('Found sequenceID and cloneID twice: {}-{}'.format(cloneID, seqID))

# Check that all naive sequences are there:
naiveID_heavy = naiveIDexp + '_heavy'
naiveID_light = naiveIDexp + '_light'
for clone in clone_dict.values():
    assert(naiveID_heavy in clone or naiveID_light in clone)
#    all_seqs = [d['seq'] for d in clone]
#    if not (len(set(all_seqs) == len(all_seqs))):
#        raise Exception('Please de-duplicate input.')

# Number of clones to Nest:
Nclones = len(clone_dict)
cloneID_list = [cloneID for cloneID in clone_dict.keys()]

nest = Nest()
w = SConsWrap(nest, outdir, alias_environment=env)


# Initialize our first nest level:
w.add('clone', cloneID_list)
w.add_aggregate('agg', list) # <-- for aggregating inference result from repeated simulations with the same parameters

try:
    os.mkdir(outdir)
except:
    pass


@w.add_target()
def write_clone(outdir, c):
    '''
    Write clone fasta and seq_info dict.
    '''
    outdir = outdir.rstrip('/')
    try:
        os.mkdir(outdir)
    except:
        pass

    cloneID = c['clone']
    clone = clone_dict[cloneID]
    seq_info_dict = dict()
    # Get the find all the chains defined for this clone:
    chain_list = [seq_info['chain'] for seq_info in clone.values()]
    chains = set(chain_list)
    # Assert that the pairing is fully resolved for a clone:
    if len(chains) == 2:
        assert(chain_list.count('heavy') == chain_list.count('light'))

    # Open files to write the clone sequences:
    fasta = list()
    naiveID = list()
    if 'heavy' in chains:
        try:
            os.mkdir(outdir+'/heavy')
        except:
            pass
        fnam = '{}/heavy/cloneID_{}_heavy.fa'.format(outdir, cloneID)
        fho_H = open(fnam, 'w')
        fasta.append(fnam)
        naiveID.append(cloneID + 'h')
    if 'light' in chains:
        try:
            os.mkdir(outdir+'/light')
        except:
            pass
        fnam = '{}/light/cloneID_{}_light.fa'.format(outdir, cloneID)
        fho_L = open(fnam, 'w')
        fasta.append(fnam)
        naiveID.append(cloneID + 'l')

    for seqID, seq_info in clone.items():
        # The unique ID of the sequence:
        if naiveIDexp in seqID:
            # This is necessary to keep the name string length down:
            uid = cloneID + seq_info['chain'][0]
        else:
            uid = cloneID + '_' + seqID
        # Store all meta-information about the sequence:
        seq_info_dict[uid] = seq_info

        # Write sequences to fasta file:
        if seq_info['chain'] == 'heavy':
            fho_H.write('>{}\n{}\n'.format(uid, seq_info['seq']))
        if seq_info['chain'] == 'light':
            fho_L.write('>{}\n{}\n'.format(uid, seq_info['seq']))

    # Dump info dict:
    seq_info_fnam = '{}/cloneID_{}_info_dict.p'.format(outdir, cloneID)
    with open(seq_info_fnam, 'wb') as fh_info_dict:
        pickle.dump(seq_info_dict, fh_info_dict)

    # Close file handles:
    try:
        fho_H.close()
    except:
        pass
    try:
        fho_L.close()
    except:
        pass

    return (fasta, naiveID, seq_info_fnam)

@w.add_target()
def infer(outdir, c):
    '''Now do inference on the simulation results.'''
    chains = ['heavy', 'light']
    fasta_l = c['write_clone'][0]
    naiveID_l = c['write_clone'][1]
    colormap = None
    converter = None
    res = list()
    for idx, z in enumerate(zip(fasta_l, naiveID_l)):
        fasta, naiveID = z
        old_outdir = outdir
        outdir = outdir + '/' + chains[idx]
        res.append((chains[idx], SConscript('SConscript.inference', exports='env tool_dict quick idlabel fasta outdir naiveID converter CommandRunner xarg buffarg colormap mutability substitution')))
        outdir = old_outdir

    return res


# This is mapping is using a dirty trick and overwrites the input pickled forest objects
# with new annotated forest objects and will fail if tried restarted on the same forest object twice.
@w.add_target()
def map_meta_information(outdir, c):
    '''Map meta information from the seq_info_dict, back to the collapsed forests.'''
    res = list()
    for tup in c['infer']:
        chain, infer_chain = tup
        log_name = 'map_meta_information_{}.log'.format(chain)
        tgt = CommandRunner([os.path.join(outdir, log_name)],
                            [infer_chain[0][2], c['write_clone'][2], [x[0] for x in infer_chain[1:]]],  # <--- Notice the first instance is skipped (this is the phylip run)
                            xarg + buffarg + 'python bin/map_meta_onto_tree.py --idmap ${SOURCES[0]} --meta ${SOURCES[1]} --forest_files ${SOURCES[2]} > ${TARGETS[0]}')
        res.append(tgt)

    return res



"""



@w.add_target()
def validate(outdir, c):
    '''Do validation.'''
    outputs = [os.path.join(outdir, 'validation.tsv'), # <-- this one compares different methods
               os.path.join(outdir, 'validation.log')]
    if tool_dict['gctree']:
        outputs.append(os.path.join(outdir, 'validation_gctree.tsv')) # <-- special gctree output evaluating parsimony tree ranking
    tgt = CommandRunner(outputs,
                   [c['gc_sim'][2:4]] + [x[0] for x in c['infer']],
                    xarg + buffarg + 'python bin/validation.py $SOURCES --outbase '+os.path.join(outdir, 'validation')+' > ${TARGETS[1]}')
    c['agg'].append(tgt[0])
    return tgt
"""




w.pop('clone')



"""
@w.add_target()
def inner_aggregate(outdir, c):
    '''aggregate validation results'''
    tgt = env.Command([os.path.join(outdir, 'validaggreg.tsv'),
                       os.path.join(outdir, 'validaggreg.log')],
                      c['agg'],
                      buffarg + 'python bin/validaggreg_compare.py $SOURCES --outbase '+os.path.join(outdir, 'validaggreg')+' > ${TARGETS[1]}')
    env.AlwaysBuild(tgt)
    return tgt
"""
